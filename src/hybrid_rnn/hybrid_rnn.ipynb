{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 01:30:02.232720: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-29 01:30:02.491274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 01:30:05.934028: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.039899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.040023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.043547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.043583: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.043596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.168404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.168431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-29 01:30:06.168457: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.168477: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-29 01:30:06.168989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import sys, os, numpy as np, pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, SimpleRNN, GRU\n",
    "from keras.layers import SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import keras\n",
    "keras.config.disable_traceback_filtering()\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)]) # set this to your gpu limit\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty way to change the current working directory to root (/toxic-comment-classification)\n",
    "# you should run this at least once just to be certain\n",
    "from os import chdir, path, getcwd\n",
    "for i in range(10):\n",
    "    if path.isfile(\"checkcwd\"):\n",
    "        break\n",
    "    chdir(path.pardir)\n",
    "if path.isfile(\"checkcwd\"):\n",
    "    pass\n",
    "else:\n",
    "    raise Exception(\"Something went wrong. cwd=\" + getcwd())\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import constants\n",
    "constants.MAX_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kaggle/input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "clean_data_path = 'clean_data/'\n",
    "EMBEDDING_GLOVE = f'{path}glove_embeddings/glove.6B.300d.txt'\n",
    "EMBEDDING_FT = f'{path}fasttext_embeddings/wiki-news-300d-1M.vec'\n",
    "TRAIN_DATA_FILE = f'{path}{comp}train.csv.zip'\n",
    "TEST_DATA_FILE = f'{path}{comp}test.csv.zip'\n",
    "CLEAN_TRAIN_DATA_FILE = f'{clean_data_path}data_train_cleaned_light_allcase.txt'\n",
    "CLEAN_TEST_DATA_FILE = f'{clean_data_path}data_test_cleaned_light_allcase.txt'\n",
    "SAMPLE_SUBMISSION = f'{path}{comp}sample_submission.csv.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_checkpoint/hybrid_rnn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = constants.MAX_FEATURES # some big number, bigger than number of unique words(?)\n",
    "maxlen = constants.MAXLEN # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "    \n",
    "list_sentences_train = read_from_file(CLEAN_TRAIN_DATA_FILE)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = read_from_file(CLEAN_TEST_DATA_FILE)\n",
    "\n",
    "train = train.assign(comment_text=list_sentences_train)\n",
    "test = test.assign(comment_text=list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh309/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/thanh309/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    # work with original text (before preprocessing and cleaning)\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(re.findall(r'\\S+', comment)))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "# extract features\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "# normalize features\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "\n",
    "dump(ss, save_path + 'scaler.bin', compress=True)\n",
    "\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "\n",
    "dump(tokenizer, save_path + 'tokenizer.bin', compress=True)\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_gl = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_GLOVE))\n",
    "embeddings_index_ft = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector: 300 glove + 300 fasttext + 1 allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, 601))\n",
    "\n",
    "\n",
    "# something: filler word for empty comment\n",
    "\n",
    "# word2vec of 'something'\n",
    "something_gl = embeddings_index_gl.get(\"something\")\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((601,))\n",
    "something[:300, ] = something_gl\n",
    "something[300:600, ] = something_ft\n",
    "something[600, ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1072/749203543.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  embedding_matrix[i, 600] = last_value\n"
     ]
    }
   ],
   "source": [
    "def all_caps(word: str) -> bool:\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix, i, word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None:\n",
    "        # embed word if is exists in fasttext dict\n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_vector_gl = embeddings_index_gl.get(word)\n",
    "        if embedding_vector_gl is not None:\n",
    "            embedding_matrix[i, :300] = embedding_vector_gl\n",
    "        embedding_matrix[i, 300:600] = embedding_vector_ft\n",
    "        embedding_matrix[i, 600] = last_value\n",
    "    else:\n",
    "        # embed word with filler word\n",
    "        embedding_matrix[i] = something\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embed_word(embedding_matrix, i, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver 2: 300 glove + 1 allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.zeros((nb_words, 301))\n",
    "\n",
    "\n",
    "# # something: filler word for empty comment\n",
    "\n",
    "# # word2vec of 'something'\n",
    "# something_gl = embeddings_index_gl.get(\"something\")\n",
    "# # something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "# something = np.zeros((301,))\n",
    "# something[:300, ] = something_gl\n",
    "# # something[300:600, ] = something_ft\n",
    "# something[300, ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_caps(word: str) -> bool:\n",
    "#     return len(word) > 1 and word.isupper()\n",
    "\n",
    "# def embed_word(embedding_matrix, i, word):\n",
    "#     if all_caps(word):\n",
    "#         last_value = np.array([1])\n",
    "#     else:\n",
    "#         last_value = np.array([0])\n",
    "#     embedding_vector_gl = embeddings_index_gl.get(word)\n",
    "#     if embedding_vector_gl is not None:\n",
    "#         embedding_matrix[i, :300] = embedding_vector_gl\n",
    "#         embedding_matrix[i, 300] = last_value\n",
    "#     else:\n",
    "#         # embed word with filler word\n",
    "#         embedding_matrix[i] = something\n",
    "\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features:\n",
    "#         continue\n",
    "#     embed_word(embedding_matrix, i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index_ft = None\n",
    "embeddings_index_gl = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import AUC\n",
    "\n",
    "def get_model(features, clipvalue=1., num_filters=40, dropout=0.5, embed_size=301):\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    print(inp.shape)\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x = Bidirectional(GRU(num_filters, return_sequences=True))(x)\n",
    "    \n",
    "    # Layer 5: A concatenation of maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    \n",
    "    x = concatenate([avg_pool, max_pool, features_input])\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp, features_input], outputs=outp)\n",
    "\n",
    "    adam = optimizers.Adam(clipvalue=clipvalue)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=[AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(save_path + 'hybrid.keras', monitor='val_auc', mode='max', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 450)\n",
      "(None, 450, 601)\n",
      "Epoch 1/5\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - auc: 0.9103 - loss: 0.1052\n",
      "Epoch 1: val_auc improved from -inf to 0.98444, saving model to model_checkpoint/hybrid_rnn/hybrid.keras\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 370ms/step - auc: 0.9104 - loss: 0.1052 - val_auc: 0.9844 - val_loss: 0.0489\n",
      "Epoch 2/5\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - auc: 0.9849 - loss: 0.0454\n",
      "Epoch 2: val_auc improved from 0.98444 to 0.98519, saving model to model_checkpoint/hybrid_rnn/hybrid.keras\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 368ms/step - auc: 0.9849 - loss: 0.0454 - val_auc: 0.9852 - val_loss: 0.0437\n",
      "Epoch 3/5\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - auc: 0.9861 - loss: 0.0432\n",
      "Epoch 3: val_auc improved from 0.98519 to 0.98592, saving model to model_checkpoint/hybrid_rnn/hybrid.keras\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 367ms/step - auc: 0.9861 - loss: 0.0432 - val_auc: 0.9859 - val_loss: 0.0426\n",
      "Epoch 4/5\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - auc: 0.9873 - loss: 0.0410\n",
      "Epoch 4: val_auc did not improve from 0.98592\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 366ms/step - auc: 0.9873 - loss: 0.0410 - val_auc: 0.9851 - val_loss: 0.0420\n",
      "Epoch 5/5\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - auc: 0.9887 - loss: 0.0397\n",
      "Epoch 5: val_auc improved from 0.98592 to 0.98687, saving model to model_checkpoint/hybrid_rnn/hybrid.keras\n",
      "\u001b[1m1496/1496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 366ms/step - auc: 0.9887 - loss: 0.0397 - val_auc: 0.9869 - val_loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f214341f530>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(features, embed_size=601)\n",
    "\n",
    "batch_size = constants.BATCH_SIZE\n",
    "# epochs = constants.EPOCHS\n",
    "epochs = 5\n",
    "\n",
    "gc.collect()\n",
    "model.fit([X_t, features], y, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 3s/step\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(save_path + 'hybrid.keras')\n",
    "print('ok')\n",
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "print('ok')\n",
    "predict = model.predict([X_te, test_features], batch_size=1024, verbose=1)\n",
    "print('ok')\n",
    "sample_submission[list_classes] = predict\n",
    "print('ok')\n",
    "sample_submission.to_csv(root_path + '/kaggle/working/hybrid_rnn/' + '1fold_hybrid_rnn.csv', index=False)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment and run from this line to predict with pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 10:08:47.171859: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 10:08:47.203517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 10:08:49.709323: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.728996: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.729036: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.732187: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.732235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.732245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.857060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.857085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-28 10:08:49.857105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.857126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-28 10:08:49.857151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# path = 'kaggle/input/'\n",
    "# comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "# clean_data_path = 'clean_data/'\n",
    "# EMBEDDING_GLOVE = f'{path}glove_embeddings/glove.6B.300d.txt'\n",
    "# EMBEDDING_FT = f'{path}fasttext_embeddings/wiki-news-300d-1M.vec'\n",
    "# TRAIN_DATA_FILE = f'{path}{comp}train.csv.zip'\n",
    "# TEST_DATA_FILE = f'{path}{comp}test.csv.zip'\n",
    "# CLEAN_TRAIN_DATA_FILE = f'{clean_data_path}data_train_cleaned_light_allcase.txt'\n",
    "# CLEAN_TEST_DATA_FILE = f'{clean_data_path}data_test_cleaned_light_allcase.txt'\n",
    "# SAMPLE_SUBMISSION = f'{path}{comp}sample_submission.csv.zip'\n",
    "# save_path = 'src/hybrid-rnn/'\n",
    "\n",
    "# from os import chdir, path, getcwd\n",
    "# import os\n",
    "# if getcwd().endswith(\"src\"):\n",
    "#     chdir(path.pardir)\n",
    "# if path.isfile(\"checkcwd\"):\n",
    "#     print(\"Success\")\n",
    "# else:\n",
    "#     raise Exception(\"Something went wrong. cwd=\" + getcwd())\n",
    "# root_path = os.getcwd()\n",
    "\n",
    "# import keras\n",
    "# import pandas as pd\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from joblib import load\n",
    "# from src import constants\n",
    "# import re\n",
    "\n",
    "# import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)])\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Virtual devices must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_from_file(filename):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh309/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "2024-05-28 10:09:00.395313: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 151ms/step\n"
     ]
    }
   ],
   "source": [
    "# loaded_model = keras.models.load_model(save_path + 'hybrid.keras')\n",
    "# ss = load('src/hybrid-rnn/scaler.bin')\n",
    "# tokenizer = load('src/hybrid-rnn/tokenizer.bin')\n",
    "\n",
    "# list_sentences_test = read_from_file(CLEAN_TEST_DATA_FILE)\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "# X_te = pad_sequences(list_tokenized_test, maxlen=constants.MAXLEN)\n",
    "\n",
    "# test = pd.read_csv(TEST_DATA_FILE)\n",
    "# test = test.assign(comment_text=list_sentences_test)\n",
    "\n",
    "# def add_features(df):\n",
    "#     # work with original text (before preprocessing and cleaning)\n",
    "#     df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "#     df['total_length'] = df['comment_text'].apply(len)\n",
    "#     df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "#     df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "#                                 axis=1)\n",
    "#     df['num_words'] = df['comment_text'].apply(lambda comment: len(re.findall(r'\\S+', comment)))\n",
    "#     df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "#     df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "#     return df\n",
    "\n",
    "# test = add_features(test)\n",
    "# test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "# test_features = ss.transform(test_features)\n",
    "\n",
    "\n",
    "# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "# sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "# predict = loaded_model.predict([X_te, test_features], batch_size=1024, verbose=1)\n",
    "# sample_submission[list_classes] = predict\n",
    "# sample_submission.to_csv(root_path + '/kaggle/working/' + '1fold-hybrid-rnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thanh309-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
