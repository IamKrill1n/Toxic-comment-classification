{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":8123170,"sourceType":"datasetVersion","datasetId":4800175}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\n# import visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# import preprocessing libraries\nimport re\nimport string\nimport spacy\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-15T09:25:15.030631Z","iopub.execute_input":"2024-04-15T09:25:15.031318Z","iopub.status.idle":"2024-04-15T09:25:15.058285Z","shell.execute_reply.started":"2024-04-15T09:25:15.031268Z","shell.execute_reply":"2024-04-15T09:25:15.057000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading training and test data","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:15.060805Z","iopub.execute_input":"2024-04-15T09:25:15.061969Z","iopub.status.idle":"2024-04-15T09:25:15.069661Z","shell.execute_reply.started":"2024-04-15T09:25:15.061927Z","shell.execute_reply":"2024-04-15T09:25:15.067688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndata_test = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_label = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nsample_submission = pd.read_csv('jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:15.071943Z","iopub.execute_input":"2024-04-15T09:25:15.072373Z","iopub.status.idle":"2024-04-15T09:25:18.588822Z","shell.execute_reply.started":"2024-04-15T09:25:15.072339Z","shell.execute_reply":"2024-04-15T09:25:18.587721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"text-preprocessing-tools-light/lemmatization-en.json\") as file:\n    lemmatizer = json.load(file)\n\nwith open(\"text-preprocessing-tools-light/stopwords.txt\") as file:\n    stopwords = file.read().splitlines()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.593147Z","iopub.execute_input":"2024-04-15T09:25:18.594053Z","iopub.status.idle":"2024-04-15T09:25:18.631858Z","shell.execute_reply.started":"2024-04-15T09:25:18.594023Z","shell.execute_reply":"2024-04-15T09:25:18.630847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.633399Z","iopub.execute_input":"2024-04-15T09:25:18.634285Z","iopub.status.idle":"2024-04-15T09:25:18.657467Z","shell.execute_reply.started":"2024-04-15T09:25:18.634247Z","shell.execute_reply":"2024-04-15T09:25:18.656338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examine the data (EDA)","metadata":{}},{"cell_type":"code","source":"data_train","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.659990Z","iopub.execute_input":"2024-04-15T09:25:18.660365Z","iopub.status.idle":"2024-04-15T09:25:18.677483Z","shell.execute_reply.started":"2024-04-15T09:25:18.660337Z","shell.execute_reply":"2024-04-15T09:25:18.676554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.679124Z","iopub.execute_input":"2024-04-15T09:25:18.679784Z","iopub.status.idle":"2024-04-15T09:25:18.728241Z","shell.execute_reply.started":"2024-04-15T09:25:18.679754Z","shell.execute_reply":"2024-04-15T09:25:18.726795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.729898Z","iopub.execute_input":"2024-04-15T09:25:18.730267Z","iopub.status.idle":"2024-04-15T09:25:18.785793Z","shell.execute_reply.started":"2024-04-15T09:25:18.730237Z","shell.execute_reply":"2024-04-15T09:25:18.784734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_classify = {}\nclasses = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor i, col in enumerate(classes):\n    data_classify[col + '_cmt'] = data_train[data_train[col] == 1]['comment_text'].values\n#     data_classify[col + '_cmt_len'] = [len(x) for x in data_classify[col + '_cmt']]\n#     data_classify[col + '_cmt_word_count'] = [len(x.split()) for x in data_classify[col + '_cmt']]\n#     data_classify[col + '_cmt_unique_word_count'] = [len(set(x.split())) for x in data_classify[col + '_cmt']]\n#     data_classify[col + '_cmt_stopword_count'] = [len([w for w in x.split() if w in STOPWORDS]) for x in data_classify[col + '_cmt']]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.787742Z","iopub.execute_input":"2024-04-15T09:25:18.788361Z","iopub.status.idle":"2024-04-15T09:25:18.831408Z","shell.execute_reply.started":"2024-04-15T09:25:18.788320Z","shell.execute_reply":"2024-04-15T09:25:18.829837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_classify['identity_hate_cmt'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.836909Z","iopub.execute_input":"2024-04-15T09:25:18.838034Z","iopub.status.idle":"2024-04-15T09:25:18.844836Z","shell.execute_reply.started":"2024-04-15T09:25:18.838000Z","shell.execute_reply":"2024-04-15T09:25:18.843899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_classify.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.846375Z","iopub.execute_input":"2024-04-15T09:25:18.846796Z","iopub.status.idle":"2024-04-15T09:25:18.853486Z","shell.execute_reply.started":"2024-04-15T09:25:18.846752Z","shell.execute_reply":"2024-04-15T09:25:18.852496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in classes:\n    print(col, len(data_classify[col + '_cmt']))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.857265Z","iopub.execute_input":"2024-04-15T09:25:18.857635Z","iopub.status.idle":"2024-04-15T09:25:18.863539Z","shell.execute_reply.started":"2024-04-15T09:25:18.857599Z","shell.execute_reply":"2024-04-15T09:25:18.862809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize meta features of each class","metadata":{}},{"cell_type":"code","source":"# clean_cmt = data_train[data_train[classes].sum(axis = 1) == 0]\n# clean_cmt\ndf = data_train.copy()\ndf['clean'] = (df.iloc[:,2:].sum(axis=1) == 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.864594Z","iopub.execute_input":"2024-04-15T09:25:18.864877Z","iopub.status.idle":"2024-04-15T09:25:18.949664Z","shell.execute_reply.started":"2024-04-15T09:25:18.864852Z","shell.execute_reply":"2024-04-15T09:25:18.948456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Indirect features\n\n#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\n    \nMETA_FEATURES  = ['count_sent', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent']\n\ndf['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100/df['count_word']","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:18.951461Z","iopub.execute_input":"2024-04-15T09:25:18.951853Z","iopub.status.idle":"2024-04-15T09:25:55.681406Z","shell.execute_reply.started":"2024-04-15T09:25:18.951823Z","shell.execute_reply":"2024-04-15T09:25:55.680135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rescale extreme cases","metadata":{}},{"cell_type":"code","source":"df.loc[df['count_sent']>10, 'count_sent'] = 10 \ndf.loc[df['count_word'] > 200, 'count_word'] = 200\ndf.loc[df['count_unique_word'] > 200, 'count_unique_word'] = 200\ndf.loc[df['count_letters'] > 1000, 'count_letters'] = 1000\ndf.loc[df['count_punctuations'] > 50, 'count_punctuations'] = 50\ndf.loc[df['count_words_upper'] > 30, 'count_words_upper'] = 30\ndf.loc[df['count_words_title'] > 30, 'count_words_title'] = 30\ndf.loc[df['count_stopwords'] > 100, 'count_stopwords'] = 100\ndf.loc[df['mean_word_len'] > 10, 'mean_word_len'] = 10","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:55.682955Z","iopub.execute_input":"2024-04-15T09:25:55.683304Z","iopub.status.idle":"2024-04-15T09:25:55.706443Z","shell.execute_reply.started":"2024-04-15T09:25:55.683276Z","shell.execute_reply":"2024-04-15T09:25:55.705413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.subplot(2, 2, 1)\nplt.title('Word count')\ntemp_df = pd.melt(df, value_vars=['count_word', 'count_unique_word'], id_vars='clean')\nsns.violinplot(data=temp_df, y = 'value', x = 'variable', hue = 'clean', split = True, inner = 'quart')\nplt.subplot(2, 2, 2)\nplt.title('Letter count')\ntemp_df = pd.melt(df, value_vars=['count_letters'], id_vars='clean')\nsns.violinplot(data=temp_df, y = 'value', x = 'variable', hue = 'clean', split = True, inner = 'quart')\nplt.subplot(2, 2, 3)\nplt.title('Sentence count')\ntemp_df = pd.melt(df, value_vars=['count_sent'], id_vars='clean')\nsns.violinplot(data=temp_df, y = 'value', x = 'variable', hue = 'clean', split = True, inner = 'quart')\nplt.legend()\nplt.subplot(2, 2, 4)\nsns.kdeplot(df[df.clean == 0].word_unique_percent, label=\"Bad\")\nsns.kdeplot(df[df.clean == 1].word_unique_percent, label=\"Clean\")\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:25:55.707808Z","iopub.execute_input":"2024-04-15T09:25:55.708696Z","iopub.status.idle":"2024-04-15T09:26:00.205342Z","shell.execute_reply.started":"2024-04-15T09:25:55.708665Z","shell.execute_reply":"2024-04-15T09:26:00.204247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Length","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize = (20, 15))\n# feat = '_cmt_len'\n# for i, col in enumerate(classes):\n#     plt.subplot(2, 3, i + 1)\n#     plt.title(col + feat)\n#     for i, x in enumerate(data_classify[col + feat]):\n#         if x > 1000:\n#             data_classify[col + feat][i] = 1000\n#     sns.violinplot(data_classify[col + feat])\n# # plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.206940Z","iopub.execute_input":"2024-04-15T09:26:00.207840Z","iopub.status.idle":"2024-04-15T09:26:00.213094Z","shell.execute_reply.started":"2024-04-15T09:26:00.207799Z","shell.execute_reply":"2024-04-15T09:26:00.212016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Count","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize = (20, 15))\n# feat = '_cmt_word_count'\n# for i, col in enumerate(classes):\n#     plt.subplot(2, 3, i + 1)\n#     plt.title(col + feat)\n#     for i, x in enumerate(data_classify[col + feat]):\n#         if x > 200:\n#             data_classify[col + feat][i] = 200\n#     sns.violinplot(data_classify[col + feat])\n# # plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.215049Z","iopub.execute_input":"2024-04-15T09:26:00.215389Z","iopub.status.idle":"2024-04-15T09:26:00.224835Z","shell.execute_reply.started":"2024-04-15T09:26:00.215350Z","shell.execute_reply":"2024-04-15T09:26:00.223670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unique Word Count","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize = (20, 15))\n# feat = '_cmt_unique_word_count'\n# for i, col in enumerate(classes):\n#     plt.subplot(2, 3, i + 1)\n#     plt.title(col + feat)\n#     for i, x in enumerate(data_classify[col + feat]):\n#         if x > 100:\n#             data_classify[col + feat][i] = 100\n#     sns.violinplot(data_classify[col + feat])\n# # plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.226512Z","iopub.execute_input":"2024-04-15T09:26:00.226926Z","iopub.status.idle":"2024-04-15T09:26:00.239215Z","shell.execute_reply.started":"2024-04-15T09:26:00.226868Z","shell.execute_reply":"2024-04-15T09:26:00.238098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation of features and targets","metadata":{}},{"cell_type":"code","source":"# This block uses Pearson Correlation, which doesn't work with categorical data\ncolormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data_train[classes].astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.240673Z","iopub.execute_input":"2024-04-15T09:26:00.241019Z","iopub.status.idle":"2024-04-15T09:26:00.775016Z","shell.execute_reply.started":"2024-04-15T09:26:00.240973Z","shell.execute_reply":"2024-04-15T09:26:00.773898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Create a cross-tabulation table\ncross_tab = pd.crosstab(df['toxic'], df['severe_toxic'])\n\n# Calculate the chi-square statistic and p-value\nchi2, p, _, _ = chi2_contingency(cross_tab)\n\n# Calculate Cramér's V\nn = cross_tab.sum().sum()\nv = np.sqrt(chi2 / (n * (min(cross_tab.shape) - 1)))\n\n# Print the correlation\nprint(\"Cramér's V:\", v)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.776324Z","iopub.execute_input":"2024-04-15T09:26:00.776655Z","iopub.status.idle":"2024-04-15T09:26:00.804058Z","shell.execute_reply.started":"2024-04-15T09:26:00.776627Z","shell.execute_reply":"2024-04-15T09:26:00.802935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean the text","metadata":{}},{"cell_type":"code","source":"APPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.805490Z","iopub.execute_input":"2024-04-15T09:26:00.805889Z","iopub.status.idle":"2024-04-15T09:26:00.816627Z","shell.execute_reply.started":"2024-04-15T09:26:00.805859Z","shell.execute_reply":"2024-04-15T09:26:00.815417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp = spacy.load(\"en_core_web_sm\")\n\ndef split_numbers_from_characters(text):\n    # Split numbers and characters\n    parts = re.split('(\\d+)', text)\n    \n    # If there was a number and a character part, add a space between them\n    if len(parts) > 1:\n        return ' '.join(parts)\n    \n    # If there was no number, return the original text\n    return text\n\ndef clean_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove non ascii characters\n    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n    # Remove special characters\n    # text = re.sub(r'\\W', ' ', text)\n    # Remove \\n\\r\n    text= re.sub(r'/\\\\n+|\\\\r+|\\n+|\\r+/', r' ', text)\n    # Remove leaky elements like ip,user\n    # text=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",text)\n    # Removing usernames\n    # text=re.sub(\"\\[\\[.*\\]\",\"\",text)\n    # Remove emojis\n#     text = re.sub(r'[\\U00010000-\\U0010ffff]', ' ', text, flags=re.UNICODE)\n    \n    # Remove punctuation\n    text = re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', ' ', text)\n    \n    # Remove numbers\n    text = split_numbers_from_characters(text)\n    text = re.sub(r'\\d', ' ', text)\n    # Replace appos\n    words=[APPO[word] if word in APPO else word for word in text.split()]\n    # lemmatization\n#     words = [lemmatizer.get(word, word) for word in words]\n    # Remove stopwords\n#     words = [word for word in words if word not in stopwords]\n    return ' '.join(words)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.818429Z","iopub.execute_input":"2024-04-15T09:26:00.818872Z","iopub.status.idle":"2024-04-15T09:26:00.832559Z","shell.execute_reply.started":"2024-04-15T09:26:00.818834Z","shell.execute_reply":"2024-04-15T09:26:00.831441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_classify['identity_hate_cmt']","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.834046Z","iopub.execute_input":"2024-04-15T09:26:00.834505Z","iopub.status.idle":"2024-04-15T09:26:00.848960Z","shell.execute_reply.started":"2024-04-15T09:26:00.834468Z","shell.execute_reply":"2024-04-15T09:26:00.847764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text(data_classify['identity_hate_cmt'][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.851591Z","iopub.execute_input":"2024-04-15T09:26:00.852133Z","iopub.status.idle":"2024-04-15T09:26:00.860656Z","shell.execute_reply.started":"2024-04-15T09:26:00.852094Z","shell.execute_reply":"2024-04-15T09:26:00.859781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train['comment_text'] = data_train['comment_text'].apply(clean_text)\ndata_test['comment_text'] = data_test['comment_text'].apply(clean_text)\nlabels = data_train.drop(['id', 'comment_text'], axis = 1)\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:00.861998Z","iopub.execute_input":"2024-04-15T09:26:00.862552Z","iopub.status.idle":"2024-04-15T09:26:37.187850Z","shell.execute_reply.started":"2024-04-15T09:26:00.862523Z","shell.execute_reply":"2024-04-15T09:26:37.186733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train['comment_text'].values","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.189232Z","iopub.execute_input":"2024-04-15T09:26:37.189650Z","iopub.status.idle":"2024-04-15T09:26:37.197307Z","shell.execute_reply.started":"2024-04-15T09:26:37.189607Z","shell.execute_reply":"2024-04-15T09:26:37.196079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined = pd.concat([data_train['comment_text'], data_test['comment_text']], axis=0)\n# combined.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.204746Z","iopub.execute_input":"2024-04-15T09:26:37.205348Z","iopub.status.idle":"2024-04-15T09:26:37.210088Z","shell.execute_reply.started":"2024-04-15T09:26:37.205311Z","shell.execute_reply":"2024-04-15T09:26:37.208845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd d:/python/Toxic-comment-classification","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.211528Z","iopub.execute_input":"2024-04-15T09:26:37.211928Z","iopub.status.idle":"2024-04-15T09:26:37.221471Z","shell.execute_reply.started":"2024-04-15T09:26:37.211899Z","shell.execute_reply":"2024-04-15T09:26:37.220384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_train.to_json('clean_data/data_train_cleaned_ver2.json')\n# data_test.to_json('clean_data/data_test_cleaned_ver2.json')\n# # labels.to_json('clean_data/labels.json')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.222822Z","iopub.execute_input":"2024-04-15T09:26:37.223142Z","iopub.status.idle":"2024-04-15T09:26:37.231876Z","shell.execute_reply.started":"2024-04-15T09:26:37.223116Z","shell.execute_reply":"2024-04-15T09:26:37.230737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try:\n#     data_train = pd.read_json('clean_data/data_train_cleaned.json')\n#     data_test = pd.read_json('clean_data/data_test_cleaned.json')\n#     labels = pd.read_json('clean_data/labels.json')\n# except:\n#     print('No such file')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.233464Z","iopub.execute_input":"2024-04-15T09:26:37.234318Z","iopub.status.idle":"2024-04-15T09:26:37.245999Z","shell.execute_reply.started":"2024-04-15T09:26:37.234272Z","shell.execute_reply":"2024-04-15T09:26:37.244731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the data","metadata":{}},{"cell_type":"code","source":"# from skmultilearn.model_selection import iterative_train_test_split\n# X_train, y_train, X_test, y_test = iterative_train_test_split(data_train, labels, test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.247502Z","iopub.execute_input":"2024-04-15T09:26:37.248186Z","iopub.status.idle":"2024-04-15T09:26:37.255362Z","shell.execute_reply.started":"2024-04-15T09:26:37.248141Z","shell.execute_reply":"2024-04-15T09:26:37.254203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(data_train, labels, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.256693Z","iopub.execute_input":"2024-04-15T09:26:37.257022Z","iopub.status.idle":"2024-04-15T09:26:37.317900Z","shell.execute_reply.started":"2024-04-15T09:26:37.256995Z","shell.execute_reply":"2024-04-15T09:26:37.316937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply TF_IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features = 100000, ngram_range=(1,2), min_df=3, max_df=0.9, strip_accents='unicode', use_idf=True, smooth_idf=True, sublinear_tf=True)\ntrain_term_doc = vec.fit_transform(X_train['comment_text'])\nval_term_doc = vec.transform(X_val['comment_text'])\ntest_term_doc = vec.transform(data_test['comment_text'])\nvec.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:26:37.319042Z","iopub.execute_input":"2024-04-15T09:26:37.320097Z","iopub.status.idle":"2024-04-15T09:27:50.145167Z","shell.execute_reply.started":"2024-04-15T09:26:37.320065Z","shell.execute_reply":"2024-04-15T09:27:50.143863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec.get_feature_names_out().shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:43:27.970249Z","iopub.execute_input":"2024-04-15T09:43:27.970742Z","iopub.status.idle":"2024-04-15T09:43:31.239706Z","shell.execute_reply.started":"2024-04-15T09:43:27.970702Z","shell.execute_reply":"2024-04-15T09:43:31.238552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_term_doc, test_term_doc","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:27:50.146523Z","iopub.execute_input":"2024-04-15T09:27:50.147096Z","iopub.status.idle":"2024-04-15T09:27:50.154325Z","shell.execute_reply.started":"2024-04-15T09:27:50.147062Z","shell.execute_reply":"2024-04-15T09:27:50.153468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy.sparse import csr_matrix, hstack\n# stack meta features with term-doc matrix\n# x = hstack(trn_term_doc, df[META_FEATURES]).tocsr()\nX_train = train_term_doc\nX_val = val_term_doc\ndel train_term_doc\ndel val_term_doc","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:27:50.155599Z","iopub.execute_input":"2024-04-15T09:27:50.156587Z","iopub.status.idle":"2024-04-15T09:27:50.173137Z","shell.execute_reply.started":"2024-04-15T09:27:50.156524Z","shell.execute_reply":"2024-04-15T09:27:50.171897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:27:50.174762Z","iopub.execute_input":"2024-04-15T09:27:50.175117Z","iopub.status.idle":"2024-04-15T09:27:50.183504Z","shell.execute_reply.started":"2024-04-15T09:27:50.175090Z","shell.execute_reply":"2024-04-15T09:27:50.182646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import sparse\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, solver = 'lbfgs', dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.solver = solver\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(solver = self.solver, C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:34:02.976503Z","iopub.execute_input":"2024-04-15T09:34:02.977500Z","iopub.status.idle":"2024-04-15T09:34:02.989791Z","shell.execute_reply.started":"2024-04-15T09:34:02.977465Z","shell.execute_reply":"2024-04-15T09:34:02.988605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def pr(y_i, y):\n#     p = x[y==y_i].sum(0)\n#     return (p+1) / ((y==y_i).sum()+1)\n\n# def get_mdl(y):\n#     y = y.values\n#     r = np.log(pr(1,y) / pr(0,y))\n#     model = LogisticRegression()\n#     x_nb = x.multiply(r)\n#     return model.fit(x_nb, y), r\n\n# def get_model(y):\n#     y = y.values\n#     model = MultinomialNB()\n#     return model.fit(x, y)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:27:50.200893Z","iopub.execute_input":"2024-04-15T09:27:50.201218Z","iopub.status.idle":"2024-04-15T09:27:50.213969Z","shell.execute_reply.started":"2024-04-15T09:27:50.201193Z","shell.execute_reply":"2024-04-15T09:27:50.212927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = np.zeros((len(test), len(classes)))\nmodels = []\nfor i, col in enumerate(classes):\n    print(col)\n    models.append(NbSvmClassifier(solver = 'liblinear', C=4, n_jobs=-1).fit(X_train, y_train[col]))\n    preds = models[i].predict_proba(X_val)[:,1].reshape(-1, 1)\n    print(roc_auc_score(y_val, preds))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:34:07.072116Z","iopub.execute_input":"2024-04-15T09:34:07.073179Z","iopub.status.idle":"2024-04-15T09:39:14.328447Z","shell.execute_reply.started":"2024-04-15T09:34:07.073143Z","shell.execute_reply":"2024-04-15T09:39:14.327114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = np.zeros((len(test), len(clas)))\n# model = []\n# for i, col in enumerate(classes):\n#     print(col)\n#     model.append(get_model(train_labels[col]))\n#     preds= model[i].predict_proba(val_x)[:,1].reshape(-1, 1)\n#     print(roc_auc_score(valid_labels, preds))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:29:50.889947Z","iopub.execute_input":"2024-04-15T09:29:50.890312Z","iopub.status.idle":"2024-04-15T09:29:50.895216Z","shell.execute_reply.started":"2024-04-15T09:29:50.890284Z","shell.execute_reply":"2024-04-15T09:29:50.893898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.zeros((len(data_test), len(classes)))\n\nfor i, col in enumerate(classes):\n    print(col)\n    preds[:, i] = models[i].predict_proba(test_term_doc)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:39:20.035593Z","iopub.execute_input":"2024-04-15T09:39:20.037935Z","iopub.status.idle":"2024-04-15T09:39:32.043282Z","shell.execute_reply.started":"2024-04-15T09:39:20.037882Z","shell.execute_reply":"2024-04-15T09:39:32.042166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:39:34.051013Z","iopub.execute_input":"2024-04-15T09:39:34.051417Z","iopub.status.idle":"2024-04-15T09:39:34.059149Z","shell.execute_reply.started":"2024-04-15T09:39:34.051389Z","shell.execute_reply":"2024-04-15T09:39:34.057971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submid = pd.DataFrame({'id': sample_submission[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = classes)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T09:39:36.259978Z","iopub.execute_input":"2024-04-15T09:39:36.260390Z","iopub.status.idle":"2024-04-15T09:39:38.811257Z","shell.execute_reply.started":"2024-04-15T09:39:36.260360Z","shell.execute_reply":"2024-04-15T09:39:38.810055Z"},"trusted":true},"execution_count":null,"outputs":[]}]}