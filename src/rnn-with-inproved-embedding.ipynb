{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, numpy as np, pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, SimpleRNN, GRU\n",
    "from keras.layers import SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers\n",
    "\n",
    "import keras\n",
    "keras.config.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# quick and dirty way to change the current working directory to root (/toxic-comment-classification)\n",
    "# you should run this at least once just to be certain\n",
    "from os import chdir, path, getcwd\n",
    "if getcwd().endswith(\"src\"):\n",
    "    chdir(path.pardir)\n",
    "if path.isfile(\"checkcwd\"):\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    raise Exception(\"Something went wrong. cwd=\" + getcwd())\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kaggle/input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "clean_data_path = 'clean_data/'\n",
    "EMBEDDING_GLOVE = f'{path}glove_embeddings/glove.6B.300d.txt'\n",
    "EMBEDDING_FT = f'{path}fasttext_embeddings/wiki-news-300d-1M.vec'\n",
    "TRAIN_DATA_FILE = f'{path}{comp}train.csv.zip'\n",
    "TEST_DATA_FILE = f'{path}{comp}test.csv.zip'\n",
    "CLEAN_TRAIN_DATA_FILE = f'{clean_data_path}data_train_cleaned_light_allcase.txt'\n",
    "CLEAN_TEST_DATA_FILE = f'{clean_data_path}data_test_cleaned_light_allcase.txt'\n",
    "SAMPLE_SUBMISSION = f'{path}{comp}sample_submission.csv.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000 # some big number, bigger than number of unique words\n",
    "maxlen = 900 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "    \n",
    "list_sentences_train = read_from_file(CLEAN_TRAIN_DATA_FILE)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = read_from_file(CLEAN_TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\S'\n",
      "/tmp/ipykernel_728/2478812342.py:8: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  df['num_words'] = df.comment_text.str.count('\\S+')\n",
      "/home/thanh309/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/thanh309/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    # work with original text (before preprocessing and cleaning)\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "# extract features\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "# normalize features\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_gl = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_GLOVE))\n",
    "embeddings_index_ft = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector: 300 glove + 300 fasttext + 1 allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, 601))\n",
    "\n",
    "\n",
    "# something: filler word for empty comment\n",
    "\n",
    "# word2vec of 'something'\n",
    "something_gl = embeddings_index_gl.get(\"something\")\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((601,))\n",
    "something[:300, ] = something_gl\n",
    "something[300:600, ] = something_ft\n",
    "something[600, ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_728/749203543.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  embedding_matrix[i, 600] = last_value\n"
     ]
    }
   ],
   "source": [
    "def all_caps(word: str) -> bool:\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix, i, word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None:\n",
    "        # embed word if is exists in fasttext dict\n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_vector_gl = embeddings_index_gl.get(word)\n",
    "        if embedding_vector_gl is not None:\n",
    "            embedding_matrix[i, :300] = embedding_vector_gl\n",
    "        embedding_matrix[i, 300:600] = embedding_vector_ft\n",
    "        embedding_matrix[i, 600] = last_value\n",
    "    else:\n",
    "        # embed word with filler word\n",
    "        embedding_matrix[i] = something\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embed_word(embedding_matrix, i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index_ft = None\n",
    "embeddings_index_gl = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(features, clipvalue=1., num_filters=40, dropout=0.5, embed_size=601):\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    print(inp.shape)\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x, x_copy, ignore = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    \n",
    "    x = concatenate([avg_pool, x_copy, max_pool, features_input])\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
    "    adam = optimizers.Adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "checkpoint_path = 'model_checkpoint/'\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                self.model.save_weights(checkpoint_path + \"best_weights.h5\")\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 5:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 900)\n",
      "(None, 900, 601)\n",
      "(None, 900)\n",
      "(None, 900, 601)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (functional_1_1/bidirectional_1_2/forward_gru_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 900, 80), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(features)\n\u001b[1;32m     34\u001b[0m ra_val \u001b[38;5;241m=\u001b[39m RocAucEvaluation(validation_data\u001b[38;5;241m=\u001b[39m([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkfold_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkfold_X_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkfold_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mra_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#model.load_weights(bst_model_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/thanh309-ml/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (functional_1_1/bidirectional_1_2/forward_gru_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 900, 80), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model = get_model(features)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "num_folds = 10\n",
    "\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Uncomment for out-of-fold predictions\n",
    "scores = []\n",
    "oof_predict = np.zeros((train.shape[0],6))\n",
    "\n",
    "predict = np.zeros((test.shape[0],6))\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "    \n",
    "    kfold_y_train, kfold_y_test = y[train_index], y[test_index]\n",
    "    kfold_X_train = X_t[train_index]\n",
    "    kfold_X_features = features[train_index]\n",
    "    kfold_X_valid = X_t[test_index]\n",
    "    kfold_X_valid_features = features[test_index] \n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = get_model(features)\n",
    "    \n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n",
    "    \n",
    "    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = [ra_val])\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    #model.load_weights(bst_model_path)\n",
    "    model.load_weights(checkpoint_path + \"best_weights.h5\")\n",
    "    \n",
    "    predict += model.predict([X_te, test_features], batch_size=batch_size, verbose=1) / num_folds\n",
    "    \n",
    "    gc.collect()\n",
    "    # uncomment for out of fold predictions\n",
    "    oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n",
    "    cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n",
    "    \n",
    "    scores.append(cv_score)\n",
    "    print('score: ', cv_score)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "#\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "sample_submission[list_classes] = predict\n",
    "sample_submission.to_csv(root_path + '/kaggle/working/' + 'rnn_glove_submission_thanh309.csv', index=False)\n",
    "\n",
    "# uncomment for out of fold predictions\n",
    "oof = pd.DataFrame.from_dict({'id': train['id']})\n",
    "for c in list_classes:\n",
    "   oof[c] = np.zeros(len(train))\n",
    "   \n",
    "oof[list_classes] = oof_predict\n",
    "for c in list_classes:\n",
    "   oof['prediction_' +c] = oof[c]\n",
    "oof.to_csv(root_path + '/kaggle/working/' + 'oof.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thanh309-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
