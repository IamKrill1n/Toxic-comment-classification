{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, numpy as np, pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, SimpleRNN, GRU\n",
    "from keras.layers import SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers\n",
    "\n",
    "import keras\n",
    "keras.config.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty way to change the current working directory to root (/toxic-comment-classification)\n",
    "# you should run this at least once just to be certain\n",
    "from os import chdir, path, getcwd\n",
    "if getcwd().endswith(\"src\"):\n",
    "    chdir(path.pardir)\n",
    "if path.isfile(\"checkcwd\"):\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    raise Exception(\"Something went wrong. cwd=\" + getcwd())\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kaggle/input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "clean_data_path = 'clean_data/'\n",
    "EMBEDDING_GLOVE = f'{path}glove_embeddings/glove.6B.300d.txt'\n",
    "EMBEDDING_FT = f'{path}fasttext_embeddings/wiki-news-300d-1M.vec'\n",
    "TRAIN_DATA_FILE = f'{path}{comp}train.csv.zip'\n",
    "TEST_DATA_FILE = f'{path}{comp}test.csv.zip'\n",
    "CLEAN_TRAIN_DATA_FILE = f'{clean_data_path}data_train_cleaned_light_allcase.txt'\n",
    "CLEAN_TEST_DATA_FILE = f'{clean_data_path}data_test_cleaned_light_allcase.txt'\n",
    "SAMPLE_SUBMISSION = f'{path}{comp}sample_submission.csv.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values used to get the score\n",
    "# max_features = 100000 # some big number, bigger than number of unique words(?)\n",
    "# maxlen = 450 # max number of words in a comment to use\n",
    "max_features = constants.MAX_FEATURES\n",
    "maxlen = constants.MAXLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "    \n",
    "list_sentences_train = read_from_file(CLEAN_TRAIN_DATA_FILE)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = read_from_file(CLEAN_TEST_DATA_FILE)\n",
    "\n",
    "train = train.assign(comment_text=list_sentences_train)\n",
    "test = test.assign(comment_text=list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    # work with original text (before preprocessing and cleaning)\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(re.findall(r'\\S+', comment)))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "# extract features\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "# normalize features\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_gl = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_GLOVE))\n",
    "# embeddings_index_ft = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector: 300 glove + 300 fasttext + 1 allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.zeros((nb_words, 601))\n",
    "\n",
    "\n",
    "# # something: filler word for empty comment\n",
    "\n",
    "# # word2vec of 'something'\n",
    "# something_gl = embeddings_index_gl.get(\"something\")\n",
    "# something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "# something = np.zeros((601,))\n",
    "# something[:300, ] = something_gl\n",
    "# something[300:600, ] = something_ft\n",
    "# something[600, ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_caps(word: str) -> bool:\n",
    "#     return len(word) > 1 and word.isupper()\n",
    "\n",
    "# def embed_word(embedding_matrix, i, word):\n",
    "#     embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "#     if embedding_vector_ft is not None:\n",
    "#         # embed word if is exists in fasttext dict\n",
    "#         if all_caps(word):\n",
    "#             last_value = np.array([1])\n",
    "#         else:\n",
    "#             last_value = np.array([0])\n",
    "#         embedding_vector_gl = embeddings_index_gl.get(word)\n",
    "#         if embedding_vector_gl is not None:\n",
    "#             embedding_matrix[i, :300] = embedding_vector_gl\n",
    "#         embedding_matrix[i, 300:600] = embedding_vector_ft\n",
    "#         embedding_matrix[i, 600] = last_value\n",
    "#     else:\n",
    "#         # embed word with filler word\n",
    "#         embedding_matrix[i] = something\n",
    "\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features:\n",
    "#         continue\n",
    "#     embed_word(embedding_matrix, i, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver 2: 300 glove + 1 allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, 301))\n",
    "\n",
    "\n",
    "# something: filler word for empty comment\n",
    "\n",
    "# word2vec of 'something'\n",
    "something_gl = embeddings_index_gl.get(\"something\")\n",
    "# something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((301,))\n",
    "something[:300, ] = something_gl\n",
    "# something[300:600, ] = something_ft\n",
    "something[300, ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_caps(word: str) -> bool:\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix, i, word):\n",
    "    if all_caps(word):\n",
    "        last_value = np.array([1])\n",
    "    else:\n",
    "        last_value = np.array([0])\n",
    "    embedding_vector_gl = embeddings_index_gl.get(word)\n",
    "    if embedding_vector_gl is not None:\n",
    "        embedding_matrix[i, :300] = embedding_vector_gl\n",
    "        embedding_matrix[i, 300] = last_value\n",
    "    else:\n",
    "        # embed word with filler word\n",
    "        embedding_matrix[i] = something\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embed_word(embedding_matrix, i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index_ft = None\n",
    "embeddings_index_gl = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(features, clipvalue=1., num_filters=40, dropout=0.5, embed_size=301):\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    print(inp.shape)\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x = Bidirectional(GRU(num_filters, return_sequences=True))(x)\n",
    "    \n",
    "    # Layer 5: A concatenation of maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    \n",
    "    x = concatenate([avg_pool, max_pool, features_input])\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
    "    adam = optimizers.Adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "checkpoint_path = 'model_checkpoint/'\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                self.model.save_weights(checkpoint_path + \"best.weights.h5\")\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 3:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "keras.config.disable_traceback_filtering()\n",
    "\n",
    "model = get_model(features)\n",
    "\n",
    "# values used to get the score\n",
    "# batch_size = 96\n",
    "# epochs = 10\n",
    "# num_folds = 10\n",
    "\n",
    "batch_size = constants.BATCH_SIZE\n",
    "epochs = constants.EPOCHS\n",
    "num_folds = constants.NUM_FOLDS\n",
    "\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Uncomment for out-of-fold predictions\n",
    "scores = []\n",
    "oof_predict = np.zeros((train.shape[0], 6))\n",
    "\n",
    "predict = np.zeros((test.shape[0], 6))\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "    \n",
    "    kfold_y_train, kfold_y_test = y[train_index], y[test_index]\n",
    "    kfold_X_train = X_t[train_index]\n",
    "    kfold_X_features = features[train_index]\n",
    "    kfold_X_valid = X_t[test_index]\n",
    "    kfold_X_valid_features = features[test_index] \n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = get_model(features)\n",
    "    \n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid, kfold_X_valid_features], kfold_y_test), interval = 1)\n",
    "    \n",
    "    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = [ra_val])\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    #model.load_weights(bst_model_path)\n",
    "    model.load_weights(checkpoint_path + \"best.weights.h5\")\n",
    "    \n",
    "    predict += model.predict([X_te, test_features], batch_size=batch_size, verbose=1) / num_folds\n",
    "    \n",
    "    gc.collect()\n",
    "    # uncomment for out of fold predictions\n",
    "    oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features], batch_size=batch_size, verbose=1)\n",
    "    cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n",
    "    \n",
    "    scores.append(cv_score)\n",
    "    print('score: ', cv_score)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "#\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "sample_submission[list_classes] = predict\n",
    "sample_submission.to_csv(root_path + '/kaggle/working/' + '4_sub.csv', index=False)\n",
    "\n",
    "# uncomment for out of fold predictions\n",
    "oof = pd.DataFrame.from_dict({'id': train['id']})\n",
    "for c in list_classes:\n",
    "   oof[c] = np.zeros(len(train))\n",
    "   \n",
    "oof[list_classes] = oof_predict\n",
    "oof.to_csv(root_path + '/kaggle/working/' + '4_oof.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thanh309-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
